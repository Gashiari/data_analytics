{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f68ae5",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054b14d",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba4eb3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspaces/data_analytics/Week_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vscode/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Import only once\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Current working directory\n",
    "print('Current working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252684a",
   "metadata": {},
   "source": [
    "## Defining documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8057467",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = \"Artificial Intelligence is transforming industries globally.\"\n",
    "d2 = \"Natural Language Processing is a subset of AI.\"\n",
    "d3 = \"Machine Learning algorithms are key to AI applications.\"\n",
    "\n",
    "documents = [d1, d2, d3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fadda5",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "#### Steps:\n",
    "- Text to lowercase\n",
    "- Removing punctuations\n",
    "- Tokenization\n",
    "- Removal of stop words\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e649b8",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2666c8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Documents:\n",
      "[['artificial', 'intelligence', 'is', 'transforming', 'industries', 'globally', '.'], ['natural', 'language', 'processing', 'is', 'a', 'subset', 'of', 'ai', '.'], ['machine', 'learning', 'algorithms', 'are', 'key', 'to', 'ai', 'applications', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenization\n",
    "tokenized_documents = [word_tokenize(doc.lower()) for doc in documents]\n",
    "print(\"Tokenized Documents:\")\n",
    "print(tokenized_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837286f",
   "metadata": {},
   "source": [
    "### Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90067406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents after Stopword Removal:\n",
      "[['artificial', 'intelligence', 'transforming', 'industries', 'globally'], ['natural', 'language', 'processing', 'subset', 'ai'], ['machine', 'learning', 'algorithms', 'key', 'ai', 'applications']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stopword removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_documents = [\n",
    "    [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    for tokens in tokenized_documents\n",
    "]\n",
    "print(\"Documents after Stopword Removal:\")\n",
    "print(filtered_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad590183",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "410fed5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Documents:\n",
      "[['artificial', 'intelligence', 'transforming', 'industry', 'globally'], ['natural', 'language', 'processing', 'subset', 'ai'], ['machine', 'learning', 'algorithm', 'key', 'ai', 'application']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_documents = [\n",
    "    [lemmatizer.lemmatize(word) for word in tokens] for tokens in filtered_documents\n",
    "]\n",
    "print(\"Lemmatized Documents:\")\n",
    "print(lemmatized_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce7d3e0",
   "metadata": {},
   "source": [
    "### Combine tokens back into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62ae4658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Documents:\n",
      "['artificial intelligence transforming industry globally', 'natural language processing subset ai', 'machine learning algorithm key ai application']\n"
     ]
    }
   ],
   "source": [
    "# Combine tokens back into sentences\n",
    "preprocessed_documents = [\" \".join(tokens) for tokens in lemmatized_documents]\n",
    "print(\"Preprocessed Documents:\")\n",
    "print(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ad6de",
   "metadata": {},
   "source": [
    "## Redefine the text corpus (pre-processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08a3cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = preprocessed_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198cc6d0",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ead679d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Features:\n",
      "['ai' 'algorithm' 'application' 'artificial' 'globally' 'industry'\n",
      " 'intelligence' 'key' 'language' 'learning' 'machine' 'natural'\n",
      " 'processing' 'subset' 'transforming']\n",
      "Unigram Matrix:\n",
      "[[0 0 0 1 1 1 1 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 1 0 0 1 1 1 0]\n",
      " [1 1 1 0 0 0 0 1 0 1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Unigram Document-Term Matrix\n",
    "vectorizer_unigram = CountVectorizer(ngram_range=(1,1))\n",
    "dtm_unigram = vectorizer_unigram.fit_transform(corpus)\n",
    "\n",
    "print(\"Unigram Features:\")\n",
    "print(vectorizer_unigram.get_feature_names_out())\n",
    "print(\"Unigram Matrix:\")\n",
    "print(dtm_unigram.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417feb3a",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4eb33ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Features:\n",
      "['ai application' 'algorithm key' 'artificial intelligence'\n",
      " 'industry globally' 'intelligence transforming' 'key ai'\n",
      " 'language processing' 'learning algorithm' 'machine learning'\n",
      " 'natural language' 'processing subset' 'subset ai'\n",
      " 'transforming industry']\n",
      "Bigram Matrix:\n",
      "[[0 0 1 1 1 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 1 0 0 1 1 1 0]\n",
      " [1 1 0 0 0 1 0 1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Bigram Document-Term Matrix\n",
    "vectorizer_bigram = CountVectorizer(ngram_range=(2,2))\n",
    "dtm_bigram = vectorizer_bigram.fit_transform(corpus)\n",
    "\n",
    "print(\"Bigram Features:\")\n",
    "print(vectorizer_bigram.get_feature_names_out())\n",
    "print(\"Bigram Matrix:\")\n",
    "print(dtm_bigram.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846a236",
   "metadata": {},
   "source": [
    "## Term frequency-inverse document frequency (TF-IDF)\n",
    "- For details see: https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854fa81",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9ff38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Features:\n",
      "['ai' 'algorithm' 'application' 'artificial' 'globally' 'industry'\n",
      " 'intelligence' 'key' 'language' 'learning' 'machine' 'natural'\n",
      " 'processing' 'subset' 'transforming']\n",
      "TF Matrix:\n",
      "[[0 0 0 1 1 1 1 0 0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 0 1 0 0 1 1 1 0]\n",
      " [1 1 1 0 0 0 0 1 0 1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Term Frequency (TF) Matrix\n",
    "tf_vectorizer = CountVectorizer()\n",
    "tf_matrix = tf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"TF Features:\")\n",
    "print(tf_vectorizer.get_feature_names_out())\n",
    "print(\"TF Matrix:\")\n",
    "print(tf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91dae3d",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5fe31336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF Features:\n",
      "['ai' 'algorithm' 'application' 'artificial' 'globally' 'industry'\n",
      " 'intelligence' 'key' 'language' 'learning' 'machine' 'natural'\n",
      " 'processing' 'subset' 'transforming']\n",
      "IDF Matrix:\n",
      "[1.28768207 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.69314718 1.69314718]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Inverse Document Frequency (IDF)\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "idf_matrix = tfidf_vectorizer.idf_\n",
    "\n",
    "print(\"IDF Features:\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"IDF Matrix:\")\n",
    "print(idf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc493eae",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c5ae575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "[[0.         0.         0.         0.4472136  0.4472136  0.4472136\n",
      "  0.4472136  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.4472136 ]\n",
      " [0.35543247 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.46735098 0.         0.         0.46735098\n",
      "  0.46735098 0.46735098 0.        ]\n",
      " [0.32200242 0.42339448 0.42339448 0.         0.         0.\n",
      "  0.         0.42339448 0.         0.42339448 0.42339448 0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Matrix\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b0f38",
   "metadata": {},
   "source": [
    "## Part-of-Speach (POS) tagging\n",
    "For meaning of POS-tags see: https://pythonexamples.org/nltk-pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5c8c05c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Artificial', 'intelligence', 'is', 'transforming', 'industries', 'and', 'improving', 'efficiency', '.']\n",
      "POS Tags: [('Artificial', 'JJ'), ('intelligence', 'NN'), ('is', 'VBZ'), ('transforming', 'VBG'), ('industries', 'NNS'), ('and', 'CC'), ('improving', 'VBG'), ('efficiency', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vscode/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Define your text\n",
    "text = \"Artificial intelligence is transforming industries and improving efficiency.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# POS Tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(\"POS Tags:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332ba3b5",
   "metadata": {},
   "source": [
    "### Explanation of Five POS Tags\n",
    "\n",
    "1. **NN**: Noun, singular. A word representing a singular noun.\n",
    "2. **JJ**: Adjective. Describes a property or quality of a noun.\n",
    "3. **VBZ**: Verb, 3rd person singular present. A verb in the present tense, third person singular.\n",
    "4. **VBG**: Verb, gerund or present participle. A verb in the gerund or present participle form.\n",
    "5. **CC**: Coordinating conjunction. Connects words, phrases, or clauses that are grammatically equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1243de",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each submitted notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "017357b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "POSIX\n",
      "Linux | 6.5.0-1025-azure\n",
      "Datetime: 2024-12-15 16:44:00\n",
      "Python Version: 3.11.10\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
